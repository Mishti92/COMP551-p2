{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are implementing a knn-classifier with L_p distance\n",
    "# this is our first implementation which builds a distance matrix\n",
    "# it becomes very inefficient when the dimension of dataset is big\n",
    "\n",
    "def distanceLp(v1, v2, p):\n",
    "    return np.power(np.absolute(np.sum((v1 - v2)) ** p), (1./p))\n",
    "\n",
    "def distanceL2(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "\n",
    "def distMatrix(train_X, test_X):\n",
    "    # we build a matrix of size (len(test_X), len(train_X))\n",
    "    # such that each row i is the distances from test[i] to all training points \n",
    "    dist_matrix = np.zeros((test_X.shape[0], train_X.shape[0]))\n",
    "    for i in range(test_X.shape[0]):\n",
    "        for j in range(train_X.shape[0]):\n",
    "            #print test_X[i], train_X[j]\n",
    "            if type(test_X)== type(np.array([])):\n",
    "                dist_matrix[i][j] = distanceL2(test_X[i], train_X[j])\n",
    "            else:\n",
    "                dist_matrix[i][j] = distanceL2(test_X[i].toarray(), train_X[j].toarray())\n",
    "    return  dist_matrix\n",
    "\n",
    "def k_nearest_neighbors(dist_matrix,k):\n",
    "    # this function builds a matrix of size (len(dist_matrix), k)\n",
    "    # for each test[i], the ith row in the matrix represents the index of k nearest neighbors\n",
    "    dist_matrix_largest_k = np.zeros((len(dist_matrix), k))\n",
    "    #print dist_matrix_largest_k \n",
    "    for i in range(len(dist_matrix)):\n",
    "        #print 'i=',i \n",
    "        dist_matrix_largest_k[i] = np.argpartition(dist_matrix[i],k)[:k] \n",
    "        #print np.argpartition(dist_matrix[i],k)[:k] \n",
    "        #print dist_matrix_largest_k[i]\n",
    "    return dist_matrix_largest_k.astype('int')\n",
    "    \n",
    "def find_majority(votes):\n",
    "    vote_count = Counter(votes)\n",
    "    top_one = vote_count.most_common(1)\n",
    "    return top_one[0][0]\n",
    "\n",
    "def knn_classifier(X_train, X_test, Y_train, k):\n",
    "    dist_M = distMatrix(X_train, X_test)\n",
    "    print 'construct distance matrix done'\n",
    "    dist_M_k =  k_nearest_neighbors(dist_M, k)\n",
    "    print 'find k nearest neubours matrix done'\n",
    "    results = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        k_index = dist_M_k[i]\n",
    "        results.append(find_majority(Y_train[k_index])) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct distance matrix done\n",
      "find k nearest neubours matrix done\n",
      "0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuedong/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/yuedong/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/yuedong/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/yuedong/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# use iris dataset for algorithm design test\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "m=0.8*len(X)\n",
    "X_train = X[:m]\n",
    "X_test = X[m:]\n",
    "Y_train = Y[:m]\n",
    "Y_test = Y[m:]\n",
    "\n",
    "## test our implementation \n",
    "pred = knn_classifier(X_train, X_test, Y_train,5)\n",
    "print accuracy_score(pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read X, Y, final_test done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'hockey': 13994,\n",
       "         'movies': 14847,\n",
       "         'nba': 12325,\n",
       "         'news': 13986,\n",
       "         'nfl': 13392,\n",
       "         'politics': 13205,\n",
       "         'soccer': 14224,\n",
       "         'worldnews': 14027})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv \n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import HTMLParser\n",
    "from nltk import word_tokenize\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import pickle\n",
    "from sklearn import svm, linear_model, naive_bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.read_csv(\"X_train.csv\")['0']\n",
    "final_test = pd.read_csv(\"X_test.csv\")['0']\n",
    "Y = pd.read_csv(\"Y_train.csv\")['0']\n",
    "print 'read X, Y, final_test done'\n",
    "#type(X)\n",
    "\n",
    "from collections import Counter\n",
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_vec(X_train,X_test,final_test):\n",
    "    print (\"Using Tfidf Vectorizer\")\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_features=15000)\n",
    "    #\n",
    "    train_data_features = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    final_test_tfidf = vectorizer.transform(final_test)\n",
    "    \n",
    "    #train_data_features = train_data_features.toarray()\n",
    "    #X_test_tfidf = X_test_tfidf.toarray()\n",
    "    #final_test_tfidf = final_test_tfidf.toarray()\n",
    "    return train_data_features,X_test_tfidf, final_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tfidf Vectorizer\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.3, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, test_tfidf= tfidf_vec(X_train, X_test, final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pred = knn_classifier(X_train_tfidf[1:1000], X_test_tfidf[1:1000], y_train[1:1000].as_matrix(),3)\n",
    "\n",
    "print 'run-time:', time.time()- start_time \n",
    "\n",
    "print 'overall accuracy', accuracy_score(y_test[1:1000].as_matrix(), pred)\n",
    "#print(classification_report(y_test[1:1000], pred))  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# knn classifier from scikit learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_tfidf[1:100], Y[1:100]) \n",
    "pred_knn_sklean = knn.predict(X_test_tfidf[1:100])\n",
    "print accuracy_score(y_test[1:100],pred_knn_sklean)\n",
    "print(classification_report(y_test[1:100], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidf_vec_for_testing(X_train,final_test):\n",
    "    print (\"Using Tfidf Vectorizer\")\n",
    "    vectorizer = TfidfVectorizer(max_features=500)\n",
    "\n",
    "    train_data_features = vectorizer.fit_transform(X_train)\n",
    "    final_test_tfidf = vectorizer.transform(final_test)\n",
    "    \n",
    "    return train_data_features, final_test_tfidf\n",
    "\n",
    "X_train_tfidf, test_tfidf= tfidf_vec_for_testing(X, final_test)\n",
    "pred_final = knn(X_train_tfidf, test_tfidf, y_test,5)\n",
    "print 'overall accuracy', accuracy_score(y_test, predicted)\n",
    "print(classification_report(y_test, predicted))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a more efficient way to implement knn without using building distance matrix\n",
    "# we are implementing a knn-classifier with the euclidean distance\n",
    "\n",
    "def distanceLp(v1, v2, p):\n",
    "    return np.power(np.sum((np.absolute(v1 - v2)) ** p), (1./p))\n",
    "    print \n",
    "def distanceL2(v1, v2):\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "\n",
    "def distVector(train_X, test_X_one_instance, p):\n",
    "    # a vector of distances from a test point to all train points\n",
    "\n",
    "    dist_vector = np.zeros(train_X.shape[0])\n",
    "    \n",
    "    for i in range(train_X.shape[0]):\n",
    "        \n",
    "        if type(test_X_one_instance)== type(np.array([])):\n",
    "            # vectors are numpy array\n",
    "            dist_vector[i] = distanceLp(test_X_one_instance, train_X[i], p)\n",
    "            \n",
    "        else:\n",
    "            # vectors are sparse matrix\n",
    "            dist_vector[i] = distanceLp(test_X_one_instance.toarray(), train_X[i].toarray(), p)\n",
    "    #print dist_vector\n",
    "    return  dist_vector\n",
    "\n",
    "def k_nearest_neighbors_v(distVector,k):\n",
    "    \n",
    "    dist_vector_largest_k = np.argpartition(distVector,k)[:k] \n",
    "        \n",
    "    return  dist_vector_largest_k.astype('int')\n",
    "    \n",
    "def find_majority(votes):\n",
    "    vote_count = Counter(votes)\n",
    "    top_one = vote_count.most_common(1)\n",
    "    return top_one[0][0]\n",
    "\n",
    "def knn_classifier_v(X_train, X_test, Y_train, k, p):\n",
    "    \n",
    "    results = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        dist_v = distVector(X_train, X_test[i], p)\n",
    "        \n",
    "        dist_v_k_index = k_nearest_neighbors_v(dist_v, k)\n",
    "        \n",
    "        results.append(find_majority(Y_train.as_matrix()[dist_v_k_index])) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pred = knn_classifier_v(X_train_tfidf[1:100], X_test_tfidf[1:100], y_train[1:100],20, 2)\n",
    "\n",
    "print 'run-time:', time.time()- start_time \n",
    "print 'overall accuracy', accuracy_score(y_test[1:100], pred)\n",
    "#print(classification_report(y_test[1:1000], pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [1,2,3,5,10,15,20]:\n",
    "    for j in [1,2,3,5]:\n",
    "        pred = knn_classifier_v(X_train_tfidf[1:10000], X_test_tfidf[1:10000], y_train[1:10000],i,j)\n",
    "        print 'k=',i,'p=',j, 'overall accuracy', accuracy_score(y_test[1:10000], pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
